"""
JSON Helper v2.0 - User Guide and Migration Documentation

This guide explains how to use the new generic json_helper_v2.py for test
reporting, migrate from the old version, and scale to new test types.
"""

# =============================================================================
# 1. OVERVIEW
# =============================================================================

"""
JSON Helper v2.0 is a generic, extensible framework for generating
standardized test reports in JSON format. It's designed to:

✓ Support arbitrary test metrics and types (not just frame_gap/CRC)
✓ Scale from simple single-test scripts to complex multi-test suites
✓ Maintain backward compatibility with existing tests
✓ Integrate seamlessly with the ingestion pipeline (SQLite → Dashboard)
✓ Provide type safety via dataclasses and optional validation
✓ Enable future extensibility without modifying core library

Key Features:
- MetricRegistry: Define and validate metrics
- Flexible TestEntry: Capture any metrics via Dict[str, Any]
- TestStatus enums: Standard "pass"/"fail"/"skip"/"partial"
- Category & Tags: Organize tests for dashboard filtering
- Artifacts: Track files (logs, plots, data)
- Timeseries: Reference large data files
- Environment Metadata: Capture bitstream, image, git SHA, etc.
"""

# =============================================================================
# 2. QUICK START (5 minutes)
# =============================================================================

"""
Minimal example to create a test report:

    from pathlib import Path
    from json_helper_v2 import create_report

    # Create report
    report = create_report(
        env={"orin_image": "r36.3", "fpga_bitstream": "hsb_20250125_01"}
    )

    # Add a test
    report.add_test(
        name="my_test",
        status="pass",
        duration_ms=1000.0,
        metrics={"my_metric": 42.5}
    )

    # Finalize and write
    report.finalize()
    report.write(Path("results"))

This creates results/summary.json with your test data.
"""

# =============================================================================
# 3. MIGRATION FROM v1.0 TO v2.0
# =============================================================================

"""
GOOD NEWS: v2.0 is backwards compatible!

Old Code (write_json.py):
    result = {
        "run_id": run_id,
        "timestamp": now_iso(),
        "env": {...},
        "summary": {...},
        "tests": [...],
        "timeseries": []
    }
    Path("runs/{run_id}/summary.json").write_text(json.dumps(result, indent=2))

New Code (json_helper_v2.py):
    report = create_report(env={...})
    report.add_test(...)
    report.finalize()
    report.write(Path("runs"))

Results are identical, but you get:
✓ Type safety (dataclasses)
✓ Validation (status must be "pass"/"fail"/"skip"/"partial")
✓ Extensibility (categories, tags, metric registry)
✓ Reusability (factory patterns, templates)

Simple Migration Path:
1. Replace manual dict construction with create_report()
2. Replace dict inserts with add_test()
3. Replace manual JSON serialization with write()
4. Optionally add categories/tags for dashboard organization
"""

# =============================================================================
# 4. CORE CONCEPTS
# =============================================================================

"""
4.1 METRIC REGISTRY
-------------------
The MetricRegistry documents what metrics your tests produce.
It's optional but recommended for validation and documentation.

    registry = MetricRegistry()
    
    # Pre-registered common metrics:
    # - frame_gap_ms_mean, frame_gap_ms_p95, frame_gap_ms_p99
    # - latency_ms_mean, latency_ms_p95, latency_ms_p99
    # - throughput_fps, crc_pass_rate, drops, etc.
    
    # Register custom metrics:
    registry.register(
        "power_avg_w",
        unit="W",
        scope="test",
        description="Average power consumption"
    )
    
    # Use in finalize() for validation
    report.finalize(metric_registry=registry)

Benefits:
- Auto-complete in IDEs
- Documentation for teammates
- Optional strict validation
- Metadata (units, descriptions)
"""

"""
4.2 TEST ENTRY
---------------
Each test has:
- name: Test identifier
- status: "pass" | "fail" | "skip" | "partial"
- duration_ms: Execution time
- metrics: Dict[str, Any] - arbitrary numeric data
- error_message: Failure reason
- artifacts: List[Artifact] - files generated by test
- category: For dashboard grouping (e.g., "performance", "compliance")
- tags: For detailed filtering (e.g., ["csi", "raw", "1080p60"])

Example:
    report.add_test(
        name="frame_gap_jitter",
        status="pass",
        duration_ms=12850.3,
        metrics={
            "frame_gap_ms_mean": 16.67,
            "frame_gap_ms_p95": 17.4,
            "frame_gap_ms_p99": 18.1,
            "drops": 0,
        },
        artifacts=[Artifact(type="png", path="fg_hist.png", label="Histogram")],
        category="performance",
        tags=["csi", "raw"],
    )
"""

"""
4.3 RUN REPORT (Top-Level Container)
--------------------------------------
A RunReport aggregates all test results for a single test run:

    report = create_report(
        run_id="2026-01-26_13-05-17_ab12cd",  # Auto-generated if omitted
        env={
            "orin_image": "r36.3",
            "fpga_bitstream": "hsb_20260125_01",
            "git_sha": "ab12cd3",
            "branch": "main",
            "dataset": "camA_1080p60",
            # ... add any custom fields needed
        }
    )

When finalize() is called, summary is auto-calculated:
    {
        "status": "pass|fail|skip|partial",  # Overall status
        "total_tests": 5,
        "passed": 4,
        "failed": 1,
        "skipped": 0,
        "yield_rate": 0.8,  # passed / total
        "notes": "Optional notes"  # If provided
    }
"""

"""
4.4 ARTIFACTS
--------------
Track files generated by tests (plots, logs, raw data).

    Artifact(
        type="png",           # log, png, mp4, json, parquet, csv, xml, numpy
        path="frames/fg.png", # Relative path
        label="Frame Gap",    # Human-readable name
        meta={"color": "rgb"} # Optional metadata
    )

Good for:
- Reference in dashboard drilldown
- Archival and reproducibility
- Linking tests to plots/visualizations
"""

"""
4.5 TIMESERIES DATA
--------------------
Reference large data files (e.g., parquet, CSV).
Used for trending and deep dives.

    report.add_timeseries(
        name="frame_gap_ms",
        path="metrics/frame_gap_ms.parquet",
        count=18000,  # Number of samples
        meta={"source": "orchestrator", "unit": "ms"}
    )

The dashboard can later fetch these files for visualization.
"""

# =============================================================================
# 5. PRACTICAL EXAMPLES
# =============================================================================

"""
5.1 CURRENT TESTS (Frame Gap, Latency, CRC)
--------------------------------------------

from pathlib import Path
from json_helper_v2 import create_report, Artifact

def run_frame_gap_test():
    # Setup
    report = create_report(
        env={
            "orin_image": "r36.3",
            "fpga_bitstream": "hsb_20250125_01",
            "git_sha": os.environ.get("GIT_SHA", "local"),
            "branch": os.environ.get("GIT_BRANCH", "unknown"),
            "dataset": "camA_1080p60",
        }
    )

    # Run test and collect metrics
    frame_gaps = measure_frame_gaps(cam)  # Your test code
    metrics = {
        "frame_gap_ms_mean": np.mean(frame_gaps),
        "frame_gap_ms_p95": np.percentile(frame_gaps, 95),
        "frame_gap_ms_p99": np.percentile(frame_gaps, 99),
        "drops": count_frame_drops(cam),
    }
    
    status = "pass" if metrics["frame_gap_ms_p99"] < 18.0 else "fail"

    # Add test
    report.add_test(
        name="frame_gap_jitter",
        status=status,
        duration_ms=test_duration_ms,
        metrics=metrics,
        artifacts=[
            Artifact(type="png", path="fg_hist.png", label="Frame Gap Histogram"),
            Artifact(type="parquet", path="fg_raw.parquet", label="Raw Samples"),
        ],
        category="performance",
        tags=["csi", "raw", "1080p60"],
    )

    # Save
    report.finalize()
    report.write(Path("results"))


5.2 FUTURE TESTS (Power, Thermal, Network)
--------------------------------------------

from json_helper_v2 import MetricRegistry

registry = MetricRegistry()
registry.register("power_avg_w", unit="W", scope="test")
registry.register("temp_max_c", unit="°C", scope="test")
registry.register("packet_loss_pct", unit="%", scope="test")

def run_power_and_thermal_test():
    report = create_report(...)
    
    # Your test code measures power/thermal
    power_data, thermal_data = measure_power_and_thermal(board, duration=300)
    
    report.add_test(
        name="power_and_thermal",
        status="pass" if thermal_data.max < 80 else "fail",
        duration_ms=300000.0,
        metrics={
            "power_avg_w": power_data.mean,
            "power_peak_w": power_data.max,
            "temp_max_c": thermal_data.max,
        },
        category="resource-monitoring",
        tags=["power", "thermal"],
    )
    
    report.finalize(metric_registry=registry)
    report.write(Path("results"))
"""

# =============================================================================
# 6. DASHBOARD INTEGRATION
# =============================================================================

"""
The JSON output is designed for seamless integration:

JSON Report (summary.json)
    ↓
ingestion_script.py (extracts tests, metrics, artifacts)
    ↓
SQLite Database (runs, tests, metrics, artifacts tables)
    ↓
local_browser_dashboard.py (Streamlit visualization)
    ↓
Browser KPIs, trends, drilldown

Important Fields for Dashboard:
- run_id: Unique identifier
- timestamp: Sort order
- env.fpga_bitstream, env.orin_image: Grouping/filtering
- tests[].name: Test identification
- tests[].status: Pass/fail coloring
- tests[].metrics: Metric values for charts
- tests[].category, tests[].tags: Dashboard organization
- summary.status: Overall pass/fail
- summary.yield_rate: Trending

Example dashboard queries:
- "Show yield rate trend over time grouped by bitstream"
  → Uses summary.yield_rate, env.fpga_bitstream, timestamp
- "Show latency p99 for each resolution"
  → Uses metrics.latency_ms_p99, tags["resolution"]
- "Compare power consumption across runs"
  → Uses metrics.power_avg_w, env.orin_image
"""

# =============================================================================
# 7. BEST PRACTICES
# =============================================================================

"""
7.1 NAMING CONVENTIONS
- Test names: lowercase_with_underscores (e.g., "frame_gap_jitter")
- Metric names: metric_type_statistic (e.g., "frame_gap_ms_p99")
- Categories: performance, compliance, stability, networking, resource-monitoring
- Tags: short descriptors (e.g., "csi", "raw", "1080p60", "eth")

7.2 METRIC ORGANIZATION
- Use consistent metric names across runs
- Include unit in metric name or registry (e.g., "latency_ms" not just "latency")
- For percentiles, always include p95 and p99 for trending
- Store raw data in timeseries, summaries in metrics

7.3 ERROR HANDLING
- Always set error_message when status="fail"
- Include threshold that was violated
- Example: "p99 latency 48.9ms exceeded 45ms threshold"

7.4 ARTIFACTS
- Store relative paths (e.g., "frames/hist.png" not "/tmp/...")
- Include label for dashboard tooltips
- Use consistent subdirectories (perf/, compliance/, etc.)

7.5 ENVIRONMENTS
- Always capture git_sha for reproducibility
- Include branch name for feature branches
- Add dataset identifier for multi-dataset testing
- Consider operator_graph_version for GXF version tracking

7.6 TEST CATEGORIES
Use categories to enable dashboard grouping:
- "performance": frame_gap, latency, throughput
- "compliance": CRC, data integrity
- "stability": long-running tests, edge cases
- "networking": Ethernet, PTP, packet loss
- "resource-monitoring": power, thermal, memory
"""

# =============================================================================
# 8. SCALING GUIDE
# =============================================================================

"""
As your test suite grows, consider:

1. METRIC REGISTRY
   Create a shared registry file:
   
    # test_registry.py
    registry = MetricRegistry()
    registry.register("my_custom_metric", unit="units", scope="test")
    
    # All tests use the same registry
    from test_registry import registry
    report.finalize(metric_registry=registry)

2. TEST TEMPLATES / FACTORY PATTERN
   Use factories for common test types:
   
    class PerformanceTestFactory:
        def create_frame_gap_test(self, duration, measurements):
            ...
        def create_latency_test(self, duration, measurements):
            ...

3. MULTI-SUITE AGGREGATION
   Combine multiple reports at run time:
   
    master_report = create_report(...)
    for suite_result in suite_results:
        parsed = json.loads(suite_result.read_text())
        for test in parsed["tests"]:
            master_report.add_test(**test)
    master_report.finalize()

4. CUSTOM STATUS DETERMINATION
   For complex tests, calculate status based on multiple metrics:
   
    if metrics["frame_gap_ms_p99"] > 18.0:
        status = "fail"
    elif metrics["frame_gap_ms_p99"] > 17.5:
        status = "partial"
    else:
        status = "pass"

5. CI/CD INTEGRATION
   Store reports in version control or artifact repository:
   
    gs://my-bucket/hsb-test-results/{run_id}/summary.json
    s3://my-bucket/hsb-test-results/{run_id}/summary.json
"""

# =============================================================================
# 9. TROUBLESHOOTING
# =============================================================================

"""
Q: "My metric isn't appearing in the dashboard"
A: Check that:
   1. Metric is in the JSON output (report.write())
   2. Metric name matches what ingestion_script expects
   3. Run finalize() before write()
   4. Check SQLite metrics table: SELECT * FROM metrics

Q: "Yield rate is wrong"
A: Verify:
   1. Test status is exactly "pass", "fail", "skip", or "partial"
   2. finalize() was called before write()
   3. All tests have proper status
   4. yield_rate = passed / total (skipped tests excluded)

Q: "Timeseries data isn't showing"
A: Check:
   1. Called add_timeseries() with correct path
   2. Path exists (relative to report location)
   3. count > 0
   4. Ingestion script correctly parsed it

Q: "Can't add custom metric"
A: No registration required! Just add to metrics dict:
   metrics={"my_custom_metric": 42.5}
   Registry is optional; for strict validation, use finalize(metric_registry=reg)
"""

# =============================================================================
# 10. API REFERENCE
# =============================================================================

"""
Main Classes:

create_report(run_id=None, env=None, schema_version="2.0")
  → Returns new RunReport

RunReport:
  .add_test(name, status, duration_ms, metrics=None, error_message=None,
            artifacts=None, category=None, tags=None) → TestEntry
  .add_timeseries(name, path, count, meta=None) → TimeseriesData
  .finalize(metric_registry=None)
  .write(out_dir, filename="summary.json") → Path

MetricRegistry:
  .register(name, unit=None, scope="test", description=None, meta=None)
  .get(name) → MetricDefinition | None
  .validate(name, strict=False) → bool

Artifact:
  Artifact(type, path, label=None, meta=None)

TimeseriesData:
  TimeseriesData(name, path, count, meta=None)

Enums:
  MetricScope: RUN, TEST
  ArtifactType: LOG, PNG, MP4, JSON, PARQUET, CSV, XML, NUMPY
  TestStatus: PASS, FAIL, SKIP, PARTIAL

Utility Functions:
  now_iso() → str  # Current UTC timestamp
  generate_run_id(prefix="") → str  # Auto-generate run ID
"""

print(__doc__)
